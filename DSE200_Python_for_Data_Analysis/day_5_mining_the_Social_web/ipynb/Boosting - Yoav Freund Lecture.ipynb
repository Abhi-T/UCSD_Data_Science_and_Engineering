{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Boosting\n",
    "## Yoav Freund\n",
    "### Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banter Inc. -> classification models\n",
    "\n",
    "Generative -> have class attributes, calc. mean vs vars for prob and will get normal distribution for 2 class attributes, see where distributions cross and create threshold (Bayes optimal rule)\n",
    "ie. male vs female voice pitch\n",
    "\n",
    "vs\n",
    "\n",
    "discriminitive -> threshold scan that finds number of mistakes -> determine where the least number of mistakes are & divide based on performance/lowest mistakes, draw dividing section\n",
    "\n",
    "Real data (ill-behaved) doesn't provide for easy generative approach since it divides incorrectly\n",
    "-> but using the discriminitive method on ill-behaved data provides a better threshold\n",
    "-> or use non-symmetrix distribution for both to account for outliers\n",
    "_________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional statistics -> data->stats->estimated world state->decision theory->predictions actions\n",
    "\n",
    "vs\n",
    "\n",
    "Machine Learning -> data->predictions actions\n",
    "_________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholding classifier -> very bias, systematic error; simple rule ass. w/more bias\n",
    "\n",
    "variance = error due to fluctuations; related to overfitting\n",
    "\n",
    "-> need a balance between the 2\n",
    "_________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees:\n",
    "CART tree learning alg (Breiman) uses index\n",
    "\n",
    "C4.5 tree learning alg (Ross Qinton) uses entropy\n",
    "\n",
    "-> Start at root (all data) find split that minimizes entropy, split data into 2, recurse on both children unless stoppping rule is applied\n",
    "_________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boasting Process\n",
    "1. start that each example has same weights\n",
    "2. change weights base on new rule\n",
    "3. recurse\n",
    "4. final rule: sign(a1h1 + a2h2 + ... + aThT)\n",
    "\n",
    "\n",
    "Adaboost\n",
    "-iteratively improves the weights\n",
    "-there is a weak learner that can always improve the model\n",
    "-boasting stumps can get you ~90% of where you would want to go\n",
    "-no dependence on # of weak rules that are combined\n",
    "\n",
    "By inputting data into a model, the model will already give you the weights that are necessary in terms of providing the most amount of information to the target; no need to complete feature selection (Freund doesn't believe in it)\n",
    "-> by doing feature selection, you are placing bias into the system and therefore ignoring the noise which should be captured in the model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
